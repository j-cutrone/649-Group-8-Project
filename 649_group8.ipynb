{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j-cutrone/649-Group-8-Project/blob/main/649_group8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN vs ViT Comparison for the Morphological Mosquito Classification Model"
      ],
      "metadata": {
        "id": "ddNOgvk3RcPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description:\n",
        "This code file implements a comparative deep learning pipeline to classify mosquito species using five distinct architectures: ResNet50, ConvNeXt Tiny, MobileNetV3, and Vision Transformers (ViT Base and Large). The workflow utilizes the timm library for transfer learning, applying white-balance correction and normalization to images before fine-tuning the models on a split of 85% training and 15% validation data. Following training, the code performs a comprehensive evaluation using confusion matrices, multi-class ROC curves, and classification reports to assess accuracy across 12 classes. Furthermore, it includes explainability modules that generate occlusion sensitivity maps to visualize model focus and t-SNE plots to analyze how different architectures cluster image features.\n",
        "\n",
        "The dataset is aggregated from multiple sources stored in Google Drive, labeled as \"VectorCam_images,\" covering specific species such as Anopheles (gambiae, funestus, stephensi, darlingi, nuneztovari, albimanus, coustani), Aedes (aegypti, albopictus), Culex, Mansonia, and non-mosquitoes. Specific data subsets identified include a general \"all_labels.csv\" collection, Colombian datasets for An. darlingi, nuneztovari, and albimanus, An. stephensi images from the University of Notre Dame, and An. coustani samples from Uganda. The script consolidates these varying CSVs and directory structures into a master dataframe, filtering out unused labels and organizing the files into a local directory structure for efficient loading."
      ],
      "metadata": {
        "id": "JzuxJULw0KWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Load in Packages ---\n",
        "!pip install -q timm\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision import models\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from pathlib import Path\n",
        "import timm\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import io\n",
        "import cv2\n",
        "import gc\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from torchvision import models, transforms, datasets"
      ],
      "metadata": {
        "id": "lhCPpc7oRhet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Mount Google Drive ---\n",
        "BASE_DRIVE_PATH = '/content/gdrive/MyDrive/649_group8/FinalProject/VectorCam_images'\n",
        "\n",
        "if not os.path.exists('/content/gdrive'):\n",
        "    drive.mount('/content/gdrive')\n",
        "    print(\"Drive mounted successfully.\")\n",
        "else:\n",
        "    print(\"Drive already mounted.\")\n",
        "print(f\"Base Drive Path set to: {BASE_DRIVE_PATH}\")\n",
        "if not os.path.exists(BASE_DRIVE_PATH):\n",
        "    print(f\"WARNING: BASE_DRIVE_PATH not found at {BASE_DRIVE_PATH}. Please ensure your Google Drive structure is correct.\")\n",
        "    if os.path.exists('/content/gdrive/MyDrive/649_group8/FinalProject/VectorCam_images'):\n",
        "        print(f\"Contents of /content/gdrive/MyDrive/649_group8/FinalProject/VectorCam_images: {os.listdir('/content/gdrive/MyDrive/649_group8/FinalProject/VectorCam_images')}\")\n",
        "    else:\n",
        "        print(f\"/content/gdrive/MyDrive/649_group8/FinalProject/VectorCam_images not found.\")"
      ],
      "metadata": {
        "id": "YPbDgb6aRm7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Image Preprocessing Function: White Balance Function ---\n",
        "def white_balance(img):\n",
        "    img_array = np.asarray(img).astype(float)\n",
        "\n",
        "    mean_r = np.mean(img_array[:, :, 0])\n",
        "    mean_g = np.mean(img_array[:, :, 1])\n",
        "    mean_b = np.mean(img_array[:, :, 2])\n",
        "\n",
        "    avg_gray = (mean_r + mean_g + mean_b) / 3\n",
        "\n",
        "    scale_r = avg_gray / mean_r if mean_r > 0 else 1\n",
        "    scale_g = avg_gray / mean_g if mean_g > 0 else 1\n",
        "    scale_b = avg_gray / mean_b if mean_b > 0 else 1\n",
        "\n",
        "    img_array[:, :, 0] *= scale_r\n",
        "    img_array[:, :, 2] *= scale_b\n",
        "    img_array[:, :, 1] *= scale_g\n",
        "\n",
        "    img_array = np.clip(img_array, 0, 255).astype(np.uint8)\n",
        "    return Image.fromarray(img_array)"
      ],
      "metadata": {
        "id": "uVZr-IuZR0K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Construct Master DataFrame ---\n",
        "RAW_IMAGES_PATH = os.path.join(BASE_DRIVE_PATH, 'raw_images')\n",
        "all_data = []\n",
        "master_df = pd.DataFrame()\n",
        "\n",
        "if not os.path.exists(RAW_IMAGES_PATH):\n",
        "    print(f\"CRITICAL ERROR: RAW_IMAGES_PATH not found at {RAW_IMAGES_PATH}. Please verify your Google Drive setup.\")\n",
        "    print(\"Proceeding with an empty master_df, which will likely cause subsequent steps to fail or use default values.\")\n",
        "else:\n",
        "\n",
        "    # A. Original Data\n",
        "    csv_orig = os.path.join(RAW_IMAGES_PATH, 'all_labels.csv')\n",
        "    if os.path.exists(csv_orig):\n",
        "        df = pd.read_csv(csv_orig)\n",
        "        df['species'] = pd.to_numeric(df['species'], errors='coerce')\n",
        "        df = df[~df['species'].isin([2, 4])].copy()\n",
        "        mapping = {0: 0, 1: 1, 3: 9, 5: 10, 6: 11}\n",
        "        df['species_label'] = df['species'].map(mapping)\n",
        "        df = df.dropna(subset=['species_label'])\n",
        "        for _, row in df.iterrows():\n",
        "            path = os.path.join(RAW_IMAGES_PATH, 'images_cropped_padded_original', str(row['Image_name']))\n",
        "            all_data.append({'path': path, 'label': int(row['species_label']), 'wb': False, 'src': 'orig'})\n",
        "    else:\n",
        "        print(f\"Warning: Original CSV not found at {csv_orig}\")\n",
        "\n",
        "    # B. Colombia CSV\n",
        "    csv_col = os.path.join(RAW_IMAGES_PATH, 'Colombia', 'An_Darlingi_An_Nuneztovari.csv')\n",
        "    dir_col = os.path.join(RAW_IMAGES_PATH, 'Colombia', 'Anopheles_Darlingi_Anopheles_Nuneztovari')\n",
        "    if os.path.exists(csv_col):\n",
        "        df = pd.read_csv(csv_col)\n",
        "        for _, row in df.iterrows():\n",
        "            path = os.path.join(dir_col, str(row['Image_name']))\n",
        "            sp = str(row['species_name']).lower()\n",
        "            if 'darlingi' in sp: all_data.append({'path': path, 'label': 3, 'wb': True, 'src': 'col_csv'})\n",
        "            elif 'nuneztovari' in sp: all_data.append({'path': path, 'label': 4, 'wb': True, 'src': 'col_csv'})\n",
        "    else:\n",
        "        print(f\"Warning: Colombia CSV not found at {csv_col}\")\n",
        "\n",
        "    # C. Colombia Folder (Albimanus)\n",
        "    dir_alb = os.path.join(RAW_IMAGES_PATH, 'Colombia', 'Anopheles_Albimanus')\n",
        "    if os.path.exists(dir_alb):\n",
        "        imgs = glob.glob(os.path.join(dir_alb, '*.*'))\n",
        "        imgs = [f for f in imgs if f.lower().endswith(('.jpg','.png','.jpeg'))]\n",
        "        for f in imgs:\n",
        "            all_data.append({'path': f, 'label': 2, 'wb': True, 'src': 'col_alb'})\n",
        "    else:\n",
        "        print(f\"Warning: Albimanus directory not found at {dir_alb}\")\n",
        "\n",
        "    # D. Stephensi CSV\n",
        "    csv_step = os.path.join(RAW_IMAGES_PATH, 'Stephensi', 'NotreDame_stephensi_VectorCAM_ids.csv')\n",
        "    dir_step = os.path.join(RAW_IMAGES_PATH, 'Stephensi', 'VectorCam_UND_cropped_padded')\n",
        "    if os.path.exists(csv_step):\n",
        "        df = pd.read_csv(csv_step)\n",
        "        img_col_name = next((c for c in df.columns if 'image' in c.lower()), None)\n",
        "        sp_col_name = next((c for c in df.columns if 'species' in c.lower() or 'morph' in c.lower()), None)\n",
        "\n",
        "        if img_col_name and sp_col_name and img_col_name in df.columns and sp_col_name in df.columns:\n",
        "            for _, row in df.iterrows():\n",
        "                path = os.path.join(dir_step, str(row[img_col_name]))\n",
        "                sp = str(row[sp_col_name]).lower()\n",
        "                if 'stephensi' in sp: all_data.append({'path': path, 'label': 5, 'wb': False, 'src': 'step'})\n",
        "                elif 'gambiae' in sp: all_data.append({'path': path, 'label': 1, 'wb': False, 'src': 'step'})\n",
        "        else:\n",
        "            print(f\"Warning: Stephensi CSV ({csv_step}) missing expected image ({img_col_name}) or species ({sp_col_name}) column. Skipping.\")\n",
        "    else:\n",
        "        print(f\"Warning: Stephensi CSV not found at {csv_step}\")\n",
        "\n",
        "    # E. Aedes Folders\n",
        "    for sub, lbl in [('colombia_2025_crop_pad_Aedes Aegypti', 7), ('colombia_2025_crop_pad_Aedes Albopictus', 8)]:\n",
        "        d = os.path.join(RAW_IMAGES_PATH, 'Aedes', sub)\n",
        "        if os.path.exists(d):\n",
        "            imgs = glob.glob(os.path.join(d, '*.*'))\n",
        "            imgs = [f for f in imgs if f.lower().endswith(('.jpg','.png','.jpeg'))]\n",
        "            for f in imgs:\n",
        "                all_data.append({'path': f, 'label': lbl, 'wb': True, 'src': 'aedes'})\n",
        "        else:\n",
        "            print(f\"Warning: Aedes directory not found at {d}\")\n",
        "\n",
        "    # F. Uganda Folder\n",
        "    dir_ug = os.path.join(RAW_IMAGES_PATH, 'Uganda', 'Coustani_images')\n",
        "    if os.path.exists(dir_ug):\n",
        "        imgs = glob.glob(os.path.join(dir_ug, '*.*'))\n",
        "        imgs = [f for f in imgs if f.lower().endswith(('.jpg','.png','.jpeg'))]\n",
        "        for f in imgs:\n",
        "            all_data.append({'path': f, 'label': 6, 'wb': False, 'src': 'ug'})\n",
        "    else:\n",
        "        print(f\"Warning: Uganda directory not found at {dir_ug}\")\n",
        "\n",
        "    master_df = pd.DataFrame(all_data)\n",
        "\n",
        "print(\"\\nMaster DataFrame Counts:\")\n",
        "if not master_df.empty:\n",
        "    print(master_df['label'].value_counts().sort_index())\n",
        "    print(f\"Total images in master_df: {len(master_df)}\")\n",
        "else:\n",
        "    print(\"Master DataFrame is empty. Check data paths and loading logic.\")"
      ],
      "metadata": {
        "id": "PxQiZ7r6T6D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Process and Organize Images ---\n",
        "LOCAL_DIR = 'organized_images_local'\n",
        "if os.path.exists(LOCAL_DIR): shutil.rmtree(LOCAL_DIR)\n",
        "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Processing images to local directory...\")\n",
        "processed_count = 0\n",
        "errors = []\n",
        "\n",
        "if not master_df.empty:\n",
        "    for _, row in tqdm(master_df.iterrows(), total=len(master_df)):\n",
        "        try:\n",
        "            if not os.path.exists(row['path']):\n",
        "                errors.append(f\"Source file not found: {row['path']}\")\n",
        "                continue\n",
        "\n",
        "            dest_dir = os.path.join(LOCAL_DIR, str(row['label']))\n",
        "            os.makedirs(dest_dir, exist_ok=True)\n",
        "            fname = os.path.basename(row['path'])\n",
        "            dest_path = os.path.join(dest_dir, fname)\n",
        "\n",
        "            if row['wb']:\n",
        "                with Image.open(row['path']) as img:\n",
        "                    img = img.convert('RGB')\n",
        "                    img_wb = white_balance(img)\n",
        "                    img_wb.save(dest_path)\n",
        "            else:\n",
        "                shutil.copy2(row['path'], dest_path)\n",
        "            processed_count += 1\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Error processing {row['path']}: {e}\")\n",
        "\n",
        "print(f\"Processed {processed_count} images. Errors: {len(errors)}\")\n",
        "if errors:\n",
        "    print(\"Some errors occurred during image processing (first 5):\")\n",
        "    for err in errors[:5]:\n",
        "        print(f\"- {err}\")"
      ],
      "metadata": {
        "id": "Tf0SDZRqUSQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Data Splitting ---\n",
        "TARGET_ROOT = 'split_dataset'\n",
        "if os.path.exists(TARGET_ROOT): shutil.rmtree(TARGET_ROOT)\n",
        "os.makedirs(TARGET_ROOT, exist_ok=True)\n",
        "\n",
        "train_df = pd.DataFrame()\n",
        "val_df = pd.DataFrame()\n",
        "\n",
        "if not master_df.empty:\n",
        "    # New split: 85% train, 15% validation\n",
        "    train_df, val_df = train_test_split(master_df, test_size=0.15, stratify=master_df['label'], random_state=42)\n",
        "    print(f\"Split sizes: Train={len(train_df)}, Val={len(val_df)}\")\n",
        "else:\n",
        "    print(\"Skipping data splitting: master_df is empty.\")\n",
        "\n",
        "\n",
        "def copy_split(df_to_copy, split_name):\n",
        "    if df_to_copy.empty:\n",
        "        print(f\"Skipping copying for {split_name} set: DataFrame is empty.\")\n",
        "        return\n",
        "\n",
        "    split_dir = os.path.join(TARGET_ROOT, split_name)\n",
        "    os.makedirs(split_dir, exist_ok=True)\n",
        "    for _, row in tqdm(df_to_copy.iterrows(), desc=f\"Populating {split_name}\"):\n",
        "        lbl = str(row['label'])\n",
        "        fname = os.path.basename(row['path'])\n",
        "        src = os.path.join(LOCAL_DIR, lbl, fname)\n",
        "        dst = os.path.join(split_dir, lbl, fname)\n",
        "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy2(src, dst)\n",
        "        else:\n",
        "            print(f\"Warning: Source file not found for copying to {split_name}: {src}\")\n",
        "\n",
        "copy_split(train_df, 'train')\n",
        "copy_split(val_df, 'val')\n",
        "print(\"Data splitting complete.\")"
      ],
      "metadata": {
        "id": "iAXWMt3RUX8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Calculate Normalization Statistics ---\n",
        "TRAIN_DIR = 'split_dataset/train'\n",
        "\n",
        "calc_stats_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "if os.path.exists(TRAIN_DIR):\n",
        "    train_dataset_stats = datasets.ImageFolder(root=TRAIN_DIR, transform=calc_stats_transform)\n",
        "    loader_stats = DataLoader(\n",
        "        train_dataset_stats,\n",
        "        batch_size=64,\n",
        "        shuffle=False,\n",
        "        num_workers=2)\n",
        "\n",
        "    print(f\"Calculating mean and std for {len(train_dataset_stats)} images...\")\n",
        "\n",
        "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "\n",
        "    for data, _ in tqdm(loader_stats, desc=\"Calculating Stats\"):\n",
        "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
        "        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
        "        num_batches += 1\n",
        "\n",
        "    # Final Mean: (Sum of Means) / (Number of Batches)\n",
        "    mean = channels_sum / num_batches\n",
        "\n",
        "    # Final Standard Deviation: Calculated using the formula: sqrt( Mean(x^2) - (Mean(x))^2 )\n",
        "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
        "\n",
        "    MEAN_NORM_FACTOR = [round(m.item(), 4) for m in mean]\n",
        "    STD_NORM_FACTOR = [round(s.item(), 4) for s in std]\n",
        "\n",
        "    print(f\"Calculated Mean: {MEAN_NORM_FACTOR}\")\n",
        "    print(f\"Calculated Std:  {STD_NORM_FACTOR}\")\n",
        "\n",
        "else:\n",
        "    print(f\"WARNING: Training directory not found at {TRAIN_DIR}. Cannot calculate normalization factors. Setting to ImageNet defaults.\")\n",
        "    MEAN_NORM_FACTOR = [0.485, 0.456, 0.406] # ImageNet defaults\n",
        "    STD_NORM_FACTOR = [0.229, 0.224, 0.225] # ImageNet defaults"
      ],
      "metadata": {
        "id": "6okz7hZFUqqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Configuration and Hyperparameters ---\n",
        "DRIVE_ROOT = \"/content/gdrive/MyDrive/\"\n",
        "PROJECT_DIR = os.path.join(DRIVE_ROOT, \"649_group8\")\n",
        "CHECKPOINT_DIR = os.path.join(PROJECT_DIR, \"checkpoints\")\n",
        "\n",
        "if 'NUM_CLASSES' not in locals() or NUM_CLASSES == 0:\n",
        "    print(\"WARNING: NUM_CLASSES not set from DataLoader or is 0. Setting to default 12 for model config.\")\n",
        "    NUM_CLASSES = 12\n",
        "elif NUM_CLASSES != 12:\n",
        "    print(f\"WARNING: NUM_CLASSES is {NUM_CLASSES}, but expected 12. Proceeding with {NUM_CLASSES}.\")\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "NUM_EPOCHS = 100\n",
        "PATIENCE = 7\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "Path(CHECKPOINT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
        "print(f\"Confirmed NUM_CLASSES for model config: {NUM_CLASSES}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "doh_Nl5RWGFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Model Definition and Training Functions ---\n",
        "\n",
        "# --- A. Model Initialization Function ---\n",
        "def create_model(model_name, num_classes):\n",
        "    model = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    if 'resnet' in model_name:\n",
        "        print(f\"Unfreezing layer4 for {model_name}...\")\n",
        "        # Unfreeze the last residual block (layer4)\n",
        "        if hasattr(model, 'layer4'):\n",
        "            for param in model.layer4.parameters():\n",
        "                param.requires_grad = True\n",
        "    elif 'convnext' in model_name:\n",
        "         print(f\"Unfreezing last stage for {model_name}...\")\n",
        "         if hasattr(model, 'stages'):\n",
        "             for param in model.stages[3].parameters():\n",
        "                 param.requires_grad = True\n",
        "    elif 'mobilenet' in model_name:\n",
        "        print(f\"MobilenetV3: no specific layers unfrozen apart from head.\")\n",
        "\n",
        "    n_features = model.num_features\n",
        "\n",
        "    if model_name == 'mobilenetv3_large_100':\n",
        "        print(f\"  (Original model.num_features for {model_name}: {n_features}. Forcing to 1280 for classifier compatibility.)\")\n",
        "        n_features = 1280\n",
        "\n",
        "    classifier_name = model.default_cfg['classifier']\n",
        "\n",
        "    new_head = nn.Linear(n_features, num_classes)\n",
        "    for param in new_head.parameters():\n",
        "        param.requires_grad = True\n",
        "    if '.' in classifier_name:\n",
        "        module_path, attr_name = classifier_name.rsplit('.', 1)\n",
        "        parent_module = model.get_submodule(module_path)\n",
        "        setattr(parent_module, attr_name, new_head)\n",
        "    else:\n",
        "        setattr(model, classifier_name, new_head)\n",
        "\n",
        "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "    return model, trainable_params\n",
        "\n",
        "# --- B. Training Loop Function ---\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "\n",
        "    for inputs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct_preds += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = correct_preds.double() / len(loader.dataset)\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "# --- C. Validation Loop Function ---\n",
        "def validate_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=\"Validation\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_preds += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = correct_preds.double() / len(loader.dataset)\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "# --- D. Experiment Runner Function ---\n",
        "def run_experiment(model_name, config, train_loader, val_loader, checkpoint_dir):\n",
        "    print(f\"\\n--- Starting Experiment: {model_name} ---\")\n",
        "\n",
        "    device = config['device']\n",
        "    num_classes = config['num_classes']\n",
        "\n",
        "    # Initialize model\n",
        "    model, trainable_params = create_model(model_name, num_classes)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Log parameter count\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_count = sum(p.numel() for p in trainable_params)\n",
        "    print(f\"Total Parameters: {total_params:,} | Trainable: {trainable_count:,}\")\n",
        "\n",
        "    # Setup Optimization\n",
        "    criterion = config['criterion']()\n",
        "    optimizer = config['optimizer'](trainable_params, lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "    scheduler = config['scheduler'](optimizer, step_size=config['scheduler_step'], gamma=config['scheduler_gamma'])\n",
        "\n",
        "    # Tracking\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0.0 # Track best validation accuracy\n",
        "    epochs_no_improve = 0\n",
        "    best_model_state = None\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}_best_checkpoint.pth\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
        "\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Valid Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Early Stopping & Checkpointing\n",
        "        if val_loss < best_val_loss:\n",
        "            print(f\"  Validation loss improved ({best_val_loss:.4f} -> {val_loss:.4f}). Saving model...\")\n",
        "            best_val_loss = val_loss\n",
        "            best_val_acc = val_acc # Update best accuracy\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"  No improvement. Patience: {epochs_no_improve}/{config['patience']}\")\n",
        "\n",
        "        if epochs_no_improve >= config['patience']:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n--- Experiment Finished: {model_name} ---\")\n",
        "    print(f\"Total Time: {total_time:.2f}s | Best Val Loss: {best_val_loss:.4f} | Best Val Acc: {best_val_acc:.4f}\")\n",
        "\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model, best_val_loss, best_val_acc"
      ],
      "metadata": {
        "id": "gY0gqH3sYN84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 9. Execute Training Experiments ---\n",
        "\n",
        "# --- A. Setup Checkpoint Directory ---\n",
        "CHECKPOINT_DIR = '/content/gdrive/MyDrive/649_group8/checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# --- B. Define Models and Configuration ---\n",
        "MODELS_TO_RUN = [\n",
        "    'resnet50',\n",
        "    'convnext_tiny',\n",
        "    'vit_base_patch16_224',\n",
        "    'vit_large_patch16_224',\n",
        "    'mobilenetv3_large_100']\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "experiment_config = {\n",
        "    'num_classes': NUM_CLASSES, # Sourced from DataLoader configuration\n",
        "    'device': DEVICE,\n",
        "    'criterion': nn.CrossEntropyLoss,\n",
        "    'optimizer': optim.AdamW,\n",
        "    'lr': 1e-4,\n",
        "    'weight_decay': 1e-4,\n",
        "    'scheduler': StepLR,\n",
        "    'scheduler_step': 7,\n",
        "    'scheduler_gamma': 0.1,\n",
        "    'num_epochs': 100,\n",
        "    'patience': 7,}\n",
        "\n",
        "print(f\"Starting training on device: {DEVICE}\")\n",
        "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
        "\n",
        "# --- C. Run Experiments ---\n",
        "results = {}\n",
        "\n",
        "# Ensure train_loader and val_loader are not None before running experiments\n",
        "if train_loader is None or val_loader is None:\n",
        "    print(\"CRITICAL ERROR: train_loader or val_loader is None. Cannot run experiments. Please check previous data loading steps.\")\n",
        "\n",
        "else:\n",
        "    for model_name in MODELS_TO_RUN:\n",
        "        model, best_loss, best_acc = run_experiment(\n",
        "            model_name=model_name,\n",
        "            config=experiment_config,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            checkpoint_dir=CHECKPOINT_DIR\n",
        "        )\n",
        "        results[model_name] = {'best_val_loss': best_loss, 'best_val_acc': best_acc}\n",
        "\n",
        "    # --- D. Final Summary ---\n",
        "    print(\"\\n\\n--- All Experiments Complete ---\")\n",
        "    print(\"Final Results (Best Validation Loss and Accuracy):\")\n",
        "    for model_name, metrics in results.items():\n",
        "        print(f\"  - {model_name}: Loss={metrics['best_val_loss']:.4f}, Acc={metrics['best_val_acc']:.4f}\")"
      ],
      "metadata": {
        "id": "zs5vu-laboBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 10. Final Class Definition ---\n",
        "NUM_CLASSES = 12\n",
        "\n",
        "CLASS_LABELS = {\n",
        "    0: \"Anopheles funestus\",      # Label 0: 821 counts\n",
        "    1: \"Anopheles gambiae\",       # Label 1: 1071 counts\n",
        "    2: \"Anopheles albimanus\",     # Label 2: 257 counts\n",
        "    3: \"Anopheles darlingi\",      # Label 3: 393 counts\n",
        "    4: \"Anopheles nuneztovari\",   # Label 4: 648 counts\n",
        "    5: \"Anopheles stephensi\",     # Label 5: 418 counts\n",
        "    6: \"Anopheles coustani\",      # Label 6: 111 counts\n",
        "    7: \"Aedes aegypti\",           # Label 7: 310 counts\n",
        "    8: \"Aedes albopictus\",        # Label 8: 158 counts\n",
        "    9: \"Culex\",                   # Label 9: 576 counts\n",
        "    10: \"Mansonia\",               # Label 10: 583 counts\n",
        "    11: \"Non-mosquito\"            # Label 11: 341 counts\n",
        "}\n",
        "SPECIES_NAMES = list(CLASS_LABELS.values())\n",
        "\n",
        "print(f\"Analysis set up for {NUM_CLASSES} classes with defined species names.\")"
      ],
      "metadata": {
        "id": "hEfErScXYiJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 11. Define F(x): ROC Curves, Precision Curves, Confusion Matrices ---\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, model_name, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        title = 'Normalized Confusion Matrix'\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(f'{title}\\n({model_name})')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_multiclass_roc(eval_results, class_names):\n",
        "    for model_name, data in eval_results.items():\n",
        "        true_labels = data['labels']\n",
        "        pred_probs = data['probs']\n",
        "\n",
        "        # Binarize the true labels for multi-class ROC calculation\n",
        "        from sklearn.preprocessing import label_binarize\n",
        "        y_test_binarized = label_binarize(true_labels, classes=np.arange(len(class_names)))\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        # Calculate Micro-Average ROC\n",
        "        fpr_micro, tpr_micro, _ = roc_curve(y_test_binarized.ravel(), pred_probs.ravel())\n",
        "        roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
        "        plt.plot(fpr_micro, tpr_micro,\n",
        "                 label=f'Micro-Avg ROC (AUC = {roc_auc_micro:0.2f})',\n",
        "                 color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "        # Calculate ROC for each class\n",
        "        for i in range(len(class_names)):\n",
        "            fpr, tpr, _ = roc_curve(y_test_binarized[:, i], pred_probs[:, i])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            plt.plot(fpr, tpr,\n",
        "                     label=f'Class {class_names[i]} (AUC = {roc_auc:0.2f})')\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'ROC Curve Analysis: {model_name}')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "KhN8tMvceeeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 12. Performance Reporting ---\n",
        "\n",
        "print(\"\\n--- Generating Analysis and Visualizations ---\")\n",
        "\n",
        "CLASS_NAMES = SPECIES_NAMES\n",
        "EVAL_RESULTS = {}\n",
        "\n",
        "print(\"Evaluating models on the validation set...\")\n",
        "for model_name in MODELS_TO_RUN:\n",
        "    print(f\"\\nLoading and evaluating {model_name}...\")\n",
        "\n",
        "    model, _ = create_model(model_name, NUM_CLASSES)\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{model_name}_best_checkpoint.pth\")\n",
        "\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"  -> Checkpoint not found for {model_name}. Skipping evaluation.\")\n",
        "        continue\n",
        "\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=f\"Evaluating {model_name}\", leave=False):\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    EVAL_RESULTS[model_name] = {\n",
        "        'labels': np.array(all_labels),\n",
        "        'probs': np.array(all_probs)}\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"Evaluation complete. Generating reports...\")\n",
        "\n",
        "for model_name, data in EVAL_RESULTS.items():\n",
        "    true_labels = data['labels']\n",
        "    pred_probs = data['probs']\n",
        "    pred_labels = np.argmax(pred_probs, axis=1)\n",
        "\n",
        "    print(f\"\\n###########################################\")\n",
        "    print(f\"## {model_name.upper()} CLASSIFICATION REPORT ##\")\n",
        "    print(f\"###########################################\")\n",
        "\n",
        "    report = classification_report(true_labels, pred_labels, target_names=CLASS_NAMES, zero_division=0)\n",
        "    print(report)\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    print(f\"Overall Accuracy: {accuracy:.4f}\\n\")\n",
        "\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    plot_confusion_matrix(cm, classes=CLASS_NAMES, model_name=model_name, title='Confusion Matrix (Counts)')\n",
        "\n",
        "    plot_confusion_matrix(cm, classes=CLASS_NAMES, model_name=model_name, normalize=True, title='Normalized Confusion Matrix')\n",
        "\n",
        "plot_multiclass_roc(EVAL_RESULTS, CLASS_NAMES)\n",
        "\n",
        "print(\"\\n--- All evaluation plots generated. ---\")"
      ],
      "metadata": {
        "id": "nUqHEjVZgc5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 13. Model Explainability via Occlusion Mapping ---\n",
        "\n",
        "# --- I. Define Global Constants ---\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUM_CLASSES = 12\n",
        "CHECKPOINT_DIR = '/content/gdrive/MyDrive/649_group8/checkpoints'\n",
        "\n",
        "SPECIES_NAMES = [\n",
        "    'Anopheles funestus', 'Anopheles gambiae', 'Anopheles albimanus',\n",
        "    'Anopheles darlingi', 'Anopheles nuneztovari', 'Anopheles stephensi',\n",
        "    'Anopheles coustani', 'Aedes aegypti', 'Aedes albopictus',\n",
        "    'Culex', 'Mansonia', 'Non-mosquito']\n",
        "\n",
        "# --- Define Single Input Image ---\n",
        "ANALYSIS_IMAGE = {\n",
        "    'path': '/content/organized_images_local/1/01_UNY40_20240702090942.jpg',\n",
        "    'true_idx': 1,  # Anopheles gambiae\n",
        "    'label': 'Anopheles gambiae'}\n",
        "TRUE_LABEL = ANALYSIS_IMAGE['label']\n",
        "TRUE_IDX = ANALYSIS_IMAGE['true_idx']\n",
        "TARGET_IMAGE_PATH = ANALYSIS_IMAGE['path']\n",
        "\n",
        "\n",
        "# Models to run\n",
        "MODELS_TO_RUN = [\n",
        "    'resnet50',\n",
        "    'convnext_tiny',\n",
        "    'vit_large_patch16_224',\n",
        "    'vit_base_patch16_224',\n",
        "    'mobilenetv3_large_100']\n",
        "\n",
        "# --- II. Define Transforms and Load Image ---\n",
        "\n",
        "VAL_TEST_TRANSFORM = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "plot_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "# Load and transform the single image (once)\n",
        "try:\n",
        "    img_pil = Image.open(TARGET_IMAGE_PATH).convert('RGB')\n",
        "    img_plot = plot_transform(img_pil).permute(1, 2, 0).cpu().numpy()\n",
        "    img_tensor = VAL_TEST_TRANSFORM(img_pil).unsqueeze(0).to(DEVICE)\n",
        "    print(f\"Input image '{os.path.basename(TARGET_IMAGE_PATH)}' (True: {TRUE_LABEL}) loaded.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"CRITICAL ERROR: Input Image not found at {TARGET_IMAGE_PATH}. Check your path.\")\n",
        "    exit()\n",
        "\n",
        "# --- III. Prediction Function ---\n",
        "\n",
        "def get_model_prediction(model, image_tensor, true_idx):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        probabilities = F.softmax(output, dim=1)\n",
        "\n",
        "        predicted_idx = torch.argmax(output, dim=1).item()\n",
        "        predicted_name = SPECIES_NAMES[predicted_idx]\n",
        "\n",
        "        predicted_confidence = probabilities[0, predicted_idx].item()\n",
        "\n",
        "        is_correct = (predicted_idx == true_idx)\n",
        "\n",
        "    return predicted_idx, predicted_name, predicted_confidence, is_correct\n",
        "\n",
        "# --- IV. Occlusion Mapping Function (Monitors a specific analysis_class_idx) ---\n",
        "\n",
        "def generate_occlusion_map(model, original_image_tensor, analysis_class_idx,\n",
        "                          mask_size=16, stride=8, device=DEVICE):\n",
        "    model.eval()\n",
        "    B, C, H, W = original_image_tensor.shape\n",
        "\n",
        "    with torch.no_grad():\n",
        "        original_output = model(original_image_tensor)\n",
        "        overall_predicted_idx = torch.argmax(original_output, dim=1).item()\n",
        "        baseline_score = original_output[0, analysis_class_idx].item()\n",
        "\n",
        "    rows = int(math.ceil((H - mask_size) / stride) + 1)\n",
        "    cols = int(math.ceil((W - mask_size) / stride) + 1)\n",
        "\n",
        "    heatmap = np.zeros((rows, cols))\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            x, y = i * stride, j * stride\n",
        "\n",
        "            occluded_image = original_image_tensor.clone().to(device)\n",
        "            occlusion_patch_val = 0.4485\n",
        "            occlusion_patch = torch.ones(C, mask_size, mask_size).to(device) * occlusion_patch_val\n",
        "\n",
        "            x_end, y_end = min(x + mask_size, H), min(y + mask_size, W)\n",
        "            occluded_image[0, :, x:x_end, y:y_end] = occlusion_patch[:, :x_end-x, :y_end-y]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                occluded_output = model(occluded_image)\n",
        "                occluded_score = occluded_output[0, analysis_class_idx].item()\n",
        "\n",
        "            score_drop = baseline_score - occluded_score\n",
        "            heatmap[i, j] = score_drop\n",
        "\n",
        "    heatmap = np.maximum(0, heatmap)\n",
        "\n",
        "    heatmap_resized = zoom(heatmap, (224/heatmap.shape[0], 224/heatmap.shape[1]), order=1)\n",
        "\n",
        "    if heatmap_resized.max() > 0:\n",
        "        heatmap_resized /= heatmap_resized.max()\n",
        "\n",
        "    return heatmap_resized\n",
        "\n",
        "# --- V. Execution Loop (1x2 Plot per Model) ---\n",
        "\n",
        "print(\"\\nStarting Occlusion Mapping and Classification for each model...\")\n",
        "\n",
        "for model_name in tqdm(MODELS_TO_RUN, desc=\"Processing Models\"):\n",
        "\n",
        "    # A. Load the model\n",
        "    try:\n",
        "        model, _ = create_model(model_name, NUM_CLASSES)\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{model_name}_best_checkpoint.pth\")\n",
        "\n",
        "        model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
        "        model.to(DEVICE).eval()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model {model_name}: {e}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # B. Get the model's prediction\n",
        "    predicted_idx, predicted_name, predicted_confidence, is_correct = get_model_prediction(\n",
        "        model, img_tensor, TRUE_IDX)\n",
        "\n",
        "    accuracy_color = 'green' if is_correct else 'red'\n",
        "    accuracy_status = 'CORRECT' if is_correct else 'INCORRECT'\n",
        "\n",
        "    # C. Generate the Occlusion Map for the model's PREDICTED class\n",
        "    try:\n",
        "        heatmap = generate_occlusion_map(\n",
        "            model=model,\n",
        "            original_image_tensor=img_tensor,\n",
        "            analysis_class_idx=predicted_idx # Monitor the model's actual prediction)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping visualization for {model_name} due to error during mapping: {e}\")\n",
        "        continue\n",
        "\n",
        "    # D. Create the 1x2 Plot\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    fig.suptitle(\n",
        "        f\"Model: {model_name.upper()} | Prediction: {predicted_name} \"\n",
        "        f\"| Status: {accuracy_status}\",\n",
        "        fontsize=14,\n",
        "        color=accuracy_color)\n",
        "\n",
        "    # --- Plot 1: Input Image ---\n",
        "    axes[0].imshow(img_plot)\n",
        "    axes[0].set_title(\n",
        "        f\"Input Image (True: {TRUE_LABEL})\",\n",
        "        fontsize=10)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # --- Plot 2: Occlusion Map ---\n",
        "    axes[1].imshow(img_plot)\n",
        "    axes[1].imshow(heatmap, cmap='jet', alpha=0.6)\n",
        "    axes[1].set_title(\n",
        "        f\"Saliency Map for '{predicted_name}'\\nConf: {predicted_confidence:.4f}\",\n",
        "        fontsize=10)\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.9])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "G41vd9Urg754"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 14. Interactive Explainability Tool Methods ---\n",
        "\n",
        "class SuppressOutput:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        self._original_stderr = sys.stderr\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "        sys.stderr = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "        sys.stderr = self._original_stderr\n",
        "\n",
        "# --- I. Model Configuration ---\n",
        "MODELS_CONFIG = [\n",
        "    {'name': 'resnet50'},\n",
        "    {'name': 'convnext_tiny'},\n",
        "    {'name': 'mobilenetv3_large_100'},\n",
        "    {'name': 'vit_base_patch16_224'},\n",
        "    {'name': 'vit_large_patch16_224'}]\n",
        "\n",
        "MODEL_ARCHITECTURES = {\n",
        "    'resnet50': {\n",
        "        'type': 'Deep CNN',\n",
        "        'description': \"Loading ResNet50...\"\n",
        "    },\n",
        "    'convnext_tiny': {\n",
        "        'type': 'Modern CNN',\n",
        "        'description': \"Loading ConvNeXt Tiny...\"\n",
        "    },\n",
        "    'mobilenetv3_large_100': {\n",
        "        'type': 'Lightweight CNN',\n",
        "        'description': \"Loading MobileNetV3...\"\n",
        "    },\n",
        "    'vit_base_patch16_224': {\n",
        "        'type': 'Vision Transformer (ViT)',\n",
        "        'description': \"Loading ViT Base...\"\n",
        "    },\n",
        "    'vit_large_patch16_224': {\n",
        "        'type': 'Vision Transformer (ViT)',\n",
        "        'description': \"Loading ViT Large...\"\n",
        "    }}\n",
        "\n",
        "# --- II. Parameter Counts ---\n",
        "MODEL_PARAMS = {\n",
        "    'resnet50': 25.6,       # M\n",
        "    'convnext_tiny': 28.6,  # M\n",
        "    'mobilenetv3_large_100': 5.5, # M\n",
        "    'vit_base_patch16_224': 86.8, # M\n",
        "    'vit_large_patch16_224': 307.3 # M}\n",
        "\n",
        "# --- III. Transforms ---\n",
        "VAL_TEST_TRANSFORM = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "# --- IV. Widget Components ---\n",
        "uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
        "process_btn = widgets.Button(description=\"Run Comparative Analysis\", button_style='success', icon='check')\n",
        "out_display = widgets.Output()\n",
        "\n",
        "# --- V. Event Handler ---\n",
        "def on_process_click(b):\n",
        "    out_display.clear_output()\n",
        "\n",
        "    if not uploader.value:\n",
        "        with out_display:\n",
        "            print(\"Please upload an image first!\")\n",
        "        return\n",
        "\n",
        "    vals = uploader.value\n",
        "    fname = list(vals.keys())[0]\n",
        "    content = vals[fname]['content']\n",
        "\n",
        "    try:\n",
        "        pil_img = Image.open(io.BytesIO(content)).convert('RGB')\n",
        "        pil_img = pil_img.resize((512, 512))\n",
        "    except Exception as e:\n",
        "        with out_display:\n",
        "            print(f\"Error reading or resizing image: {e}\")\n",
        "        return\n",
        "\n",
        "    input_tensor = VAL_TEST_TRANSFORM(pil_img).unsqueeze(0).to(DEVICE)\n",
        "    plot_img_pil = pil_img.resize((224, 224))\n",
        "    orig_rgb = np.array(plot_img_pil)\n",
        "    orig_bgr = cv2.cvtColor(orig_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    with out_display:\n",
        "        print(f\"Analyzing: {fname}\")\n",
        "        print(\"---\")\n",
        "\n",
        "    model_count = len(MODELS_CONFIG)\n",
        "    for i, config in enumerate(tqdm(MODELS_CONFIG, desc=\"Overall Analysis Progress\")):\n",
        "        m_name = config['name']\n",
        "        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"{m_name}_best_checkpoint.pth\")\n",
        "\n",
        "        params = MODEL_PARAMS.get(m_name, 'N/A')\n",
        "\n",
        "        with out_display:\n",
        "            arch_info = MODEL_ARCHITECTURES.get(m_name, {'description': f\"Loading {m_name}. No detailed architecture description available.\"})\n",
        "            print(f\"\\n[STEP {i+1}/{model_count}] {arch_info['description']} (Parameters: {params}M)\")\n",
        "\n",
        "        total_start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            steps = 3\n",
        "            with tqdm(total=steps, desc=f\"  {m_name}\", leave=False) as pbar:\n",
        "\n",
        "                with SuppressOutput():\n",
        "                    model, _ = create_model(m_name, NUM_CLASSES)\n",
        "\n",
        "                load_start = time.time()\n",
        "                model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "                model.to(DEVICE).eval()\n",
        "                load_time = time.time() - load_start\n",
        "                pbar.update(1) # Step 1: Model Loaded\n",
        "\n",
        "                # 1. Inference\n",
        "                inference_start = time.time()\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(input_tensor)\n",
        "                    probs = torch.softmax(outputs, dim=1)\n",
        "                    top_prob, top_idx = torch.max(probs, 1)\n",
        "                inference_time = time.time() - inference_start\n",
        "\n",
        "                pred_idx = top_idx.item()\n",
        "                pred_name = SPECIES_NAMES[pred_idx] if 'SPECIES_NAMES' in globals() and 0 <= pred_idx < len(SPECIES_NAMES) else f\"Idx {pred_idx}\"\n",
        "                pbar.update(1)\n",
        "\n",
        "                # 2. Generate Occlusion Map\n",
        "                cam_start = time.time()\n",
        "                cam_map, _ = generate_occlusion_map_single(model, input_tensor.clone(), device=DEVICE)\n",
        "                cam_time = time.time() - cam_start\n",
        "\n",
        "                total_run_time = time.time() - total_start_time\n",
        "\n",
        "                results.append({\n",
        "                    'name': m_name,\n",
        "                    'pred': pred_name,\n",
        "                    'conf': top_prob.item(),\n",
        "                    'map': cam_map,\n",
        "                    'runtime': total_run_time,\n",
        "                    'load_time': load_time,\n",
        "                    'inference_time': inference_time,\n",
        "                    'cam_time': cam_time\n",
        "                })\n",
        "                pbar.update(1)\n",
        "\n",
        "                del model\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "             total_run_time = time.time() - total_start_time\n",
        "             with out_display:\n",
        "                if os.path.exists(ckpt_path):\n",
        "                     print(f\"  -> Critical Error during inference/loading: {e}\")\n",
        "                else:\n",
        "                     print(f\"  -> Checkpoint not found. Skipping.\")\n",
        "\n",
        "        # --- Print Runtime Analysis ---\n",
        "        if 'runtime' in locals():\n",
        "            with out_display:\n",
        "                print(f\"  -> Runtime Analysis (Total: {total_run_time:.3f}s)\")\n",
        "                print(f\"    - Model Loading: {load_time:.3f}s\")\n",
        "                print(f\"    - Inference:     {inference_time:.3f}s\")\n",
        "                print(f\"    - Occlusion Map: {cam_time:.3f}s\")\n",
        "    # --- End Model Processing ---\n",
        "\n",
        "\n",
        "    # --- VI. Plotting (Single 1x6 Row) ---\n",
        "    if results:\n",
        "        num_models = len(results)\n",
        "        num_plots = 1 + num_models\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_plots, figsize=(4 * num_plots, 5))\n",
        "\n",
        "        primary_pred = results[0]['pred']\n",
        "\n",
        "        fig.suptitle(\n",
        "            f\"Comparative Occlusion Maps (Prediction: {primary_pred})\",\n",
        "            fontsize=16\n",
        "        )\n",
        "\n",
        "        with out_display:\n",
        "\n",
        "            axes[0].imshow(orig_rgb)\n",
        "            axes[0].set_title(f\"Input Image (224x224)\", fontsize=10)\n",
        "            axes[0].axis('off')\n",
        "\n",
        "            for col_idx, result in enumerate(results):\n",
        "\n",
        "                title_color = 'black'\n",
        "\n",
        "                heatmap = result['map']\n",
        "                heatmap_cv = cv2.resize(heatmap, (224, 224))\n",
        "                heatmap_cv = np.uint8(255 * heatmap_cv)\n",
        "                heatmap_cv = cv2.applyColorMap(heatmap_cv, cv2.COLORMAP_JET)\n",
        "\n",
        "                overlay = cv2.addWeighted(orig_bgr, 0.6, heatmap_cv, 0.4, 0)\n",
        "                overlay_rgb = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # 2. Plot\n",
        "                ax = axes[col_idx + 1]\n",
        "                ax.imshow(overlay_rgb)\n",
        "\n",
        "                ax.set_title(\n",
        "                    f\"{result['name']}\\n\"\n",
        "                    f\"Pred: {result['pred']} (Conf: {result['conf']:.2%})\\n\"\n",
        "                    f\"Run: {result['runtime']:.2f}s | {MODEL_PARAMS.get(result['name'], 'N/A')}M Params\",\n",
        "                    fontsize=9,\n",
        "                    color=title_color)\n",
        "                ax.axis('off')\n",
        "\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.9])\n",
        "            display(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "process_btn.on_click(on_process_click)\n",
        "\n",
        "# --- VII. Display UI ---\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h2 style='font-size: 24pt;'>Mosquito Species Classifier</h2>\"),\n",
        "    widgets.HBox([\n",
        "        widgets.Label(\"Upload Image:\", style={'description_width': 'initial', 'font_size': '14pt'}),\n",
        "        uploader\n",
        "    ]),\n",
        "    process_btn,\n",
        "    out_display]))"
      ],
      "metadata": {
        "id": "q0LKQc4ihf0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 15. TSNE plots ---\n",
        "print(\"\\nStarting t-SNE Analysis (Combined + Individual Mode)...\")\n",
        "\n",
        "CHECKPOINT_DIR = '/content/gdrive/MyDrive/649_group8/checkpoints'\n",
        "OUTPUT_DIR = '/content/gdrive/MyDrive/649_group8/FinalProject/tsne_plots_new'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Saving all plots to: {OUTPUT_DIR}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_configs = {\n",
        "    'MobileNetV3':   {'file': 'mobilenetv3_large_100_best_checkpoint.pth', 'timm_name': 'mobilenetv3_large_100'},\n",
        "    'ViT-Large':     {'file': 'vit_large_patch16_224_best_checkpoint.pth', 'timm_name': 'vit_large_patch16_224'},\n",
        "    'ResNet-50':     {'file': 'resnet50_best_checkpoint.pth',             'timm_name': 'resnet50'},\n",
        "    'ViT-Base':      {'file': 'vit_base_patch16_224_best_checkpoint.pth',  'timm_name': 'vit_base_patch16_224'},\n",
        "    'ConvNeXt-Tiny': {'file': 'convnext_tiny_best_checkpoint.pth',        'timm_name': 'convnext_tiny'}}\n",
        "\n",
        "def prepare_timm_model(config, num_classes, checkpoint_dir, device):\n",
        "    \"\"\"Creates model, loads weights, removes head.\"\"\"\n",
        "    try:\n",
        "        model = timm.create_model(config['timm_name'], pretrained=False, num_classes=num_classes)\n",
        "        model = model.to(device)\n",
        "\n",
        "        ckpt_path = os.path.join(checkpoint_dir, config['file'])\n",
        "        if not os.path.exists(ckpt_path):\n",
        "            print(f\"  [Error] Checkpoint not found: {ckpt_path}\")\n",
        "            return None\n",
        "\n",
        "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "        state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint\n",
        "\n",
        "        new_state_dict = {}\n",
        "        for k, v in state_dict.items():\n",
        "            name = k[7:] if k.startswith('module.') else k\n",
        "            new_state_dict[name] = v\n",
        "\n",
        "        model.load_state_dict(new_state_dict, strict=True)\n",
        "        model.reset_classifier(0)\n",
        "        model.eval()\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"  [Error] Failed to load {config['timm_name']}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_features(model, loader, device):\n",
        "    \"\"\"Extracts features from validation set.\"\"\"\n",
        "    features_list, labels_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in tqdm(loader, desc=\"Extracting\", leave=False):\n",
        "            imgs = imgs.to(device)\n",
        "            output = model(imgs)\n",
        "            if output.ndim > 2:\n",
        "                output = output.mean(dim=[-2, -1])\n",
        "            features_list.append(output.cpu().numpy())\n",
        "            labels_list.append(lbls.numpy())\n",
        "    return np.vstack(features_list), np.concatenate(labels_list)\n",
        "\n",
        "if 'val_loader' not in locals():\n",
        "    print(\"CRITICAL ERROR: val_loader missing. Please run the Data Preparation code first.\")\n",
        "else:\n",
        "    TRAIN_NUM_CLASSES = 12\n",
        "    print(f\"Assuming models trained with {TRAIN_NUM_CLASSES} classes.\")\n",
        "\n",
        "    plot_data_storage = []\n",
        "\n",
        "    # A. Loop through models to generate data and Individual Plots\n",
        "    for i, (display_name, config) in enumerate(model_configs.items()):\n",
        "        print(f\"\\n--- Processing {display_name} ---\")\n",
        "\n",
        "        # A. Load Model\n",
        "        model = prepare_timm_model(config, TRAIN_NUM_CLASSES, CHECKPOINT_DIR, device)\n",
        "\n",
        "        if model:\n",
        "            # B. Extract Features\n",
        "            X, y = extract_features(model, val_loader, device)\n",
        "\n",
        "            # C. Output Point Count\n",
        "            num_points = len(y)\n",
        "            print(f\"  > Number of data points (images): {num_points}\")\n",
        "\n",
        "            # D. Compute t-SNE\n",
        "            print(f\"  > Computing t-SNE...\")\n",
        "            curr_perp = min(30, num_points - 1) if num_points > 1 else 1\n",
        "            tsne = TSNE(n_components=2, random_state=42, perplexity=curr_perp, init='pca', learning_rate='auto')\n",
        "            X_embedded = tsne.fit_transform(X)\n",
        "\n",
        "            # Store data for combined plot\n",
        "            plot_data_storage.append({\n",
        "                'name': display_name,\n",
        "                'coords': X_embedded,\n",
        "                'labels': y\n",
        "            })\n",
        "\n",
        "            # E. Plot & Save INDIVIDUAL Figure\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            scatter = sns.scatterplot(\n",
        "                x=X_embedded[:, 0],\n",
        "                y=X_embedded[:, 1],\n",
        "                hue=y,\n",
        "                palette='tab10',\n",
        "                legend='full',\n",
        "                s=80, alpha=0.7\n",
        "            )\n",
        "            plt.title(f\"{display_name} (n={num_points})\", fontsize=16, fontweight='bold')\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., title=\"Class ID\")\n",
        "            plt.tight_layout()\n",
        "\n",
        "            indiv_path = os.path.join(OUTPUT_DIR, f\"tsne_{display_name.replace(' ', '_')}.png\")\n",
        "            plt.savefig(indiv_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"  > Individual plot saved to: {indiv_path}\")\n",
        "            plt.close() # Close to save memory\n",
        "\n",
        "            # Cleanup model\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        else:\n",
        "            print(f\"  > Skipping {display_name} due to load error.\")\n",
        "\n",
        "    # B. Generate COMBINED Plot\n",
        "    if plot_data_storage:\n",
        "        print(f\"\\n--- Generating Combined Plot for {len(plot_data_storage)} models ---\")\n",
        "\n",
        "        fig, axes = plt.subplots(1, 5, figsize=(25, 6))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, data in enumerate(plot_data_storage):\n",
        "            ax = axes[i]\n",
        "            X_emb = data['coords']\n",
        "            y_lbl = data['labels']\n",
        "            name = data['name']\n",
        "\n",
        "            sns.scatterplot(\n",
        "                x=X_emb[:, 0],\n",
        "                y=X_emb[:, 1],\n",
        "                hue=y_lbl,\n",
        "                palette='tab10',\n",
        "                legend='full' if i == 4 else False, # Legend only on last subplot\n",
        "                s=60, alpha=0.7,\n",
        "                ax=ax\n",
        "            )\n",
        "            ax.set_title(f\"{name}\\n(n={len(y_lbl)})\", fontsize=14, fontweight='bold')\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "            ax.set_xlabel('')\n",
        "            ax.set_ylabel('')\n",
        "\n",
        "        # Fix Legend on Combined Plot\n",
        "        handles, labels = axes[4].get_legend_handles_labels()\n",
        "        if handles:\n",
        "            axes[4].legend_.remove()\n",
        "            fig.legend(handles, labels, loc='center right', title='Class ID', bbox_to_anchor=(1.02, 0.5))\n",
        "\n",
        "        plt.suptitle(\"t-SNE Clustering Comparison\", fontsize=20, y=1.05)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        combined_path = os.path.join(OUTPUT_DIR, \"tsne_combined_all_models.png\")\n",
        "        plt.savefig(combined_path, bbox_inches='tight', dpi=300)\n",
        "        print(f\"  > Combined plot saved to: {combined_path}\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No models were successfully processed to create a combined plot.\")"
      ],
      "metadata": {
        "id": "Msqr5eGYjAUj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}